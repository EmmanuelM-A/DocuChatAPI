# Project Architecture â€“ DocuChatAPI

---

## Overview

The **DocuChatAPI** is a modular **FastAPI-based Retrieval-Augmented Generation 
(RAG) backend** designed to support user-managed chatbot sessions, document uploads, 
and conversational interactions with vector-based retrieval.  

The system follows a **layered architecture**:

- **API Layer** â†’ Handles HTTP requests/responses.

- **Service Layer** â†’ Encapsulates business logic (sessions, documents, messaging). 

- **Persistence Layer** â†’ Manages database storage and vector embeddings. 

- **AI Layer** â†’ RAG pipeline (retrieval + LLM response).

## High-Level Architecture

**Core Components:**

- **FastAPI API** â€“ Exposes endpoints for authentication, session management, 
document upload, chat, and usage tracking.  

- **Database (PostgresSQL + SQLModel)** â€“ Stores structured application data.

- **Vector Store (FAISS / Pinecone / Weaviate, pluggable)** â€“ Manages embeddings for 
document search.  

- **Authentication & Middleware** â€“ JWT or OAuth2 authentication for secure API access.

- **Business Logic Services** â€“ Handle session lifecycle, token accounting, and limits. 

- **LLM Integration (OpenAI / Local LLMs)** â€“ Provides intelligent responses enhanced 
by context retrieval.

## Component Breakdown

### ðŸ”¹ API Layer

- **Routers** for `auth`, `users`, `sessions`, `documents`, `messages`, `usage`.  
- **Dependencies**: Database session injection, authentication guards.  

### ðŸ”¹ Service Layer

- **Session Service** â€“ Creates chat sessions, enforces limits, retrieves history.  
- **Document Service** â€“ Handles file ingestion, text chunking, vector embedding.  
- **Message Service** â€“ Stores and retrieves chat messages.  
- **Usage Service** â€“ Tracks token usage, session count, and uploads.  

### ðŸ”¹ Persistence Layer

- **SQLModel ORM** â€“ Defines relational schemas (`User`, `ChatSession`, `Document`, 
`ChatMessage`, `UsageStats`).

- **CRUD Operations** â€“ Encapsulated per schema (e.g., `user_model.py`, `session_model.py`).  

- **Vector Store Integration** â€“ Maintains embeddings and supports similarity search.  

### ðŸ”¹ AI Layer

- **RAG Pipeline:**

  1. Retrieve top-k relevant document chunks.  
  2. Construct prompt with context + chat history.  
  3. Call LLM (OpenAI API, HuggingFace, or custom).  
  4. Stream or return assistant response.

- **User** â€“ Auth + subscription details.  

- **ChatSession** â€“ Groups related documents and messages.

- **Document** â€“ Uploads linked to sessions & users.

- **ChatMessage** â€“ Logs conversations (user â†” assistant). 

- **UsageStats** â€“ Tracks token usage and limits.

## Suggested Diagrams to Draw

### High-Level System Diagram

- Shows: API Layer â†’ Service Layer â†’ Persistence Layer â†’ AI Layer â†’ LLM.

- Request flow: **User â†’ FastAPI â†’ DB + Vector Store â†’ LLM â†’ Response**.  

### Database ER Diagram

- Entities: `User`, `ChatSession`, `Document`, `ChatMessage`, `UsageStats`.  
- Relationships:  
  - `User (1) â†’ (âˆž) ChatSession`  
  - `ChatSession (1) â†’ (âˆž) Document`  
  - `ChatSession (1) â†’ (âˆž) ChatMessage`  
  - `User (1) â†’ (1) UsageStats`  

### Sequence Diagram (Chat Request Flow)

1. User sends message.  
2. API validates authentication.  
3. Retrieve session + documents from DB/vector store.  
4. Construct RAG prompt â†’ Send to LLM.  
5. Receive assistant response.  
6. Store message + tokens in DB.  
7. Return response to user.  

---

## Scalability & Extension Points

- **Pluggable Vector Store**: Start with FAISS (local), upgrade to Pinecone/Weaviate.  
- **Authentication**: OAuth2/JWT, extendable for enterprise SSO.  
- **Task Queues** (Celery, RQ) for background document ingestion.  
- **Caching Layer** (Redis) for faster retrieval & token savings.
